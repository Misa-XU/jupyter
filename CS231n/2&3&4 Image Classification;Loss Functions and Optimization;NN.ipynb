{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN __Distance Metric__ to compare images:\n",
    "- L1 Norm: $d_1(I_1, I_2) = \\sum^p |I_1^p - I_2^p|$\n",
    "- L2 Norm: $d_1(I_1, I_2) = \\sum^p \\sqrt{(I_1^p - I_2^p)^2}$\n",
    "\n",
    "L1 depends on the choice of your coordinate system; Whereas there's no effect on L2 since a circle is similar. So if there's some special meaning, maybe somehow L1 is a more natual fit; Otherwise, L2 may be natual.\n",
    "![](images/db.png)\n",
    "It's shown that the decision boundary of L1 tends to follow the coordinate axes.\n",
    "\n",
    "KNN is never used since it's hard to densely cover pixels in such high dimensional space\n",
    "\n",
    "@FLANN\n",
    "\n",
    "@[Recognizing and Learning Object Categories](http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html)\n",
    "\n",
    "@[A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameters\n",
    "- Very Problem-dependent\n",
    "- Must try them all out and see which is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Classifier\n",
    "Only learn one template for each class\n",
    "![](images/fails.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi - SVM\n",
    "![](images/svmloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the choice of **1** is actually doesn't matter since the effect is canceled with the scale of __W__\n",
    "- The min possible value of svm Loss is **0** and max is $\\infty$\n",
    "- At the beginning, since the values of __W__ is small, the svm loss is expected to be **#nofclasses - 1**, which may be helpful when debugging\n",
    "- The loss above omit the loss at correct class to make the minimum loss be zero, if calculate it as a part of loss, the new loss will be the loss above plus 1\n",
    "- Optionally, we may square the hinge loss (adjust the trade-off between goodness and badness)\n",
    "- Loss function is to tell the algorithm what kind of error should be care about and what should trade-off against\n",
    "- Suppose a dataset is perfectly separatable, then suppose W makes L = 0, 2W also makes L = 0. So for avoiding overfitting, a regularization is a must to \"simplify\" the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: Optimization in primal. If you’re coming to this class with previous knowledge of SVMs, you may have also heard of kernels, duals, the SMO algorithm, etc. In this class (as is the case with Neural Networks in general) we will always work with the optimization objectives in their unconstrained primal form. Many of these objectives are technically not differentiable (e.g. the max(x,y) function isn’t because it has a kink when x=y), but in practice this is not a problem and it is common to use a subgradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@ [Deep Learning using Linear Support Vector Machines](https://arxiv.org/abs/1306.0239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "![](images/softmax.png)\n",
    "- Min is **0** and Max is $\\infty$\n",
    "- The initial L should be $\\log $ __#nClasses__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you’re writing code for computing the Softmax function in practice, the intermediate terms efyi and ∑jefj may be very large due to the exponentials.\n",
    "![](images/sft.png)\n",
    "where $\\log C = − \\max_j f_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences:\n",
    "The SVM interprets these as class scores and its loss function encourages the correct class to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high.\n",
    "\n",
    "The only thing SVM care is whether the correct scores is larger than a margin above the incorrect scores. Softmax will always want to drive the prob of correct class to 1. This property of SVM can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.\n",
    "\n",
    "But pratically, these tends not to make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 is preferred to spread the influence accross all X, the decision depend the entire X vector. \n",
    "    - L2 corresponses a MAP inference using a Gaussian prior on W\n",
    "- L1 has a opposite interpretation. It prefers a sparse weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bias regularization__. As we already mentioned in the Linear Classification section, it is not common to regularize the bias parameters because they do not interact with the data through multiplicative interactions, and therefore do not have the interpretation of controlling the influence of a data dimension on the final objective. However, in practical applications (and with proper data preprocessing) regularizing the bias rarely leads to significantly worse performance. This is likely because there are very few bias terms compared to all the weights, so the classifier can “afford to” use the biases if it needs them to obtain a better data loss.\n",
    "\n",
    "__Per-layer regularization__. It is not very common to regularize different layers to different amounts (except perhaps the output layer). Relatively few results regarding this idea have been published in the literature.\n",
    "\n",
    "__In practice__: It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of p=0.5 is a reasonable default, but this can be tuned on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word of caution: It is important to note that the L2 loss is much harder to optimize than a more stable loss such as Softmax. Intuitively, it requires a very fragile and specific property from the network to output exactly one correct value for each input (and its augmentations). Notice that this is not the case with Softmax, where the precise value of each score is less important: It only matters that their magnitudes are appropriate. Additionally, the L2 loss is less robust because outliers can introduce huge gradients. When faced with a regression problem, first consider if it is absolutely inadequate to quantize the output into bins. For example, if you are predicting star rating for a product, it might work much better to use 5 independent classifiers for ratings of 1-5 stars instead of a regression loss. Classification has the additional benefit that it can give you a distribution over the regression outputs, not just a single output with no indication of its confidence. If you’re certain that classification is not appropriate, use the L2 but be careful: For example, the L2 is more fragile and applying dropout in the network (especially in the layer right before the L2 loss) is not a great idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient:\n",
    "Always use analytical gradient, but numerical gradient is useful when debugging\n",
    "\n",
    "#### Minibatch:\n",
    "32/64/128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Features\n",
    "![](images/bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training process is \n",
    "![](images/tp.png)\n",
    "The difference between such process and ConvNet is in cnn, we don't need to write the rule of feature extraction by ourself; we learn it from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "“fully-connected networks” or sometimes “multi-layer perceptrons” (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pa.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mi.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/af.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
