{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN __Distance Metric__ to compare images:\n",
    "- L1 Norm: $d_1(I_1, I_2) = \\sum^p |I_1^p - I_2^p|$\n",
    "- L2 Norm: $d_1(I_1, I_2) = \\sum^p \\sqrt{(I_1^p - I_2^p)^2}$\n",
    "\n",
    "L1 depends on the choice of your coordinate system; Whereas there's no effect on L2 since a circle is similar. So if there's some special meaning, maybe somehow L1 is a more natual fit; Otherwise, L2 may be natual.\n",
    "![](images/db.png)\n",
    "It's shown that the decision boundary of L1 tends to follow the coordinate axes.\n",
    "\n",
    "KNN is never used since it's hard to densely cover pixels in such high dimensional space\n",
    "\n",
    "@FLANN\n",
    "\n",
    "@[Recognizing and Learning Object Categories](http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html)\n",
    "\n",
    "@[A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameters\n",
    "- Very Problem-dependent\n",
    "- Must try them all out and see which is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Classifier\n",
    "Only learn one template for each class\n",
    "![](images/fails.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi - SVM\n",
    "![](images/svmloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the choice of **1** is actually doesn't matter since the effect is canceled with the scale of __W__\n",
    "- The min possible value of svm Loss is **0** and max is $\\infty$\n",
    "- At the beginning, since the values of __W__ is small, the svm loss is expected to be **#nofclasses - 1**, which may be helpful when debugging\n",
    "- The loss above omit the loss at correct class to make the minimum loss be zero, if calculate it as a part of loss, the new loss will be the loss above plus 1\n",
    "- Optionally, we may square the hinge loss (adjust the trade-off between goodness and badness)\n",
    "- Loss function is to tell the algorithm what kind of error should be care about and what should trade-off against\n",
    "- Suppose a dataset is perfectly separatable, then suppose W makes L = 0, 2W also makes L = 0. So for avoiding overfitting, a regularization is a must to \"simplify\" the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: Optimization in primal. If you’re coming to this class with previous knowledge of SVMs, you may have also heard of kernels, duals, the SMO algorithm, etc. In this class (as is the case with Neural Networks in general) we will always work with the optimization objectives in their unconstrained primal form. Many of these objectives are technically not differentiable (e.g. the max(x,y) function isn’t because it has a kink when x=y), but in practice this is not a problem and it is common to use a subgradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@ [Deep Learning using Linear Support Vector Machines](https://arxiv.org/abs/1306.0239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "![](images/softmax.png)\n",
    "- Min is **0** and Max is $\\infty$\n",
    "- The initial L should be $\\log $ __#nClasses__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you’re writing code for computing the Softmax function in practice, the intermediate terms efyi and ∑jefj may be very large due to the exponentials.\n",
    "![](images/sft.png)\n",
    "where $\\log C = − \\max_j f_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences:\n",
    "The SVM interprets these as class scores and its loss function encourages the correct class to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high.\n",
    "\n",
    "The only thing SVM care is whether the correct scores is larger than a margin above the incorrect scores. Softmax will always want to drive the prob of correct class to 1. This property of SVM can intuitively be thought of as a feature: For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.\n",
    "\n",
    "But pratically, these tends not to make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 is preferred to spread the influence accross all X, the decision depend the entire X vector. \n",
    "    - L2 corresponses a MAP inference using a Gaussian prior on W\n",
    "- L1 has a opposite interpretation. It prefers a sparse weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient:\n",
    "Always use analytical gradient, but numerical gradient is useful when debugging\n",
    "\n",
    "#### Minibatch:\n",
    "32/64/128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Features\n",
    "![](images/bow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training process is \n",
    "![](images/tp.png)\n",
    "The difference between such process and ConvNet is in cnn, we don't need to write the rule of feature extraction by ourself; we learn it from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "“fully-connected networks” or sometimes “multi-layer perceptrons” (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pa.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mi.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/af.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
