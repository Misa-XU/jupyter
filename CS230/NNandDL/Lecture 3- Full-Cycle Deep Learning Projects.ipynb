{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of an ML Project:\n",
    "1. Select Problem: For example a voice activated project (via supervised learning), we need to specify the input and output\n",
    "    - X: audioclip $\\to$ Y: 1{found trigger word}\n",
    "2. Get labelled data\n",
    "    - How many days (1, 2, 3, 5, 8 ...) for collecting data\n",
    "    - How would you collect data\n",
    "    - Don't spend too much time on data collection because ML is actually a very iterative process, until we try, we never know what's hard and what's easy (unless you are really well-experienced) After rudimentary training, you will know where your algorithm fails, then go back and collect corresponding data\n",
    "3. Design model\n",
    "4. Train model (May turn back to step 2&3 )\n",
    "    - In the step 3 & 4, research is quite helpful\n",
    "    - Keep clear notes on experiments, may have a spreadsheet\n",
    "5. Test model\n",
    "6. Deploy\n",
    "    - Location:\n",
    "        - Edge (end device) Away from network latency, privacy\n",
    "        - Cloud: Easier to maintance \n",
    "    - VAD, voice activity detection\n",
    "        1. Non-ML See if volumn $\\ge \\epsilon$; Less reliable, simpler, more robust when applying to a new dataset\n",
    "        2. Train small NN/ SVM on human speech; Less robust but in pracital is a must\n",
    "        - Less parameters, more general\n",
    "    - Data change: the training data are different from which our algorithm need to perform well on\n",
    "        - New accents\n",
    "        - Different background noises\n",
    "        - New microphone\n",
    "7. Monitor\n",
    "    - Web search (the world is changing)\n",
    "    - Self-driving (different rules)\n",
    "8. Q&A: kind of statistical testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criteria to Select a Project\n",
    "- Interest\n",
    "- Availability of Data\n",
    "- Domain Knowledge\n",
    "- Utility\n",
    "- Feasibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structered Data: \n",
    "- databases of data\n",
    "- Each of the features has well defined meaning\n",
    "\n",
    "Unstructered Data: \n",
    "- Audio, images, text\n",
    "- People have natual empathy to understand unstructured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "1. Sigmoid\n",
    "    $$f(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "2. Tanh (mathematically a shifted version of the sigmoid function, almost always works better than sigmoid), it gives a sense of \"centering\", the mean of the activations will be 0, will make the learning of next layer easier. But in binary classification, the sigmoid function may be better since it makes more sence to output a value within 0 and 1\n",
    "    $$f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "    $$f'(z) = 1 - f^2(z)$$\n",
    "#### Weakness of these two above:\n",
    "The gradient becomes very small when z is either very large or very small, which may slow down the gradient descent\n",
    "\n",
    "3. Relu:\n",
    "    $$a = \\max(0, z)$$\n",
    "4. Leaky Relu: It usually works better than ReLU\n",
    "    $$a  = \\max(0.01z, z) \\,e.g.$$\n",
    "#### The network will learn faster if either of these two above is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Initialization:\n",
    "the params should be small like in the degree of 0.01 (for a shallow nn) since like for example, applying tanh, if w large, then z large, a large, the gradient will be small, then learning is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are functions that only very deep nn can learn and shallower models often fails to command. And it's difficult to predict in advance that exactly how deep a neural network need to be applied. So we may try a shallower nn (one or two) first, and consider the number of layers as a hyperparameters, evaulating them on hold-out cross validation.\n",
    "\n",
    "#### Notations:\n",
    "- L: the number of layers\n",
    "- $n^{[\\ell]}$: the number of units in layer $\\ell$\n",
    "- $a^{[\\ell]}$: activations in layer $\\ell$\n",
    "- $W^{[\\ell]}, b^{[\\ell]}$: weights for computing __$z^{[\\ell]}$__ for example: $z^{[\\ell]} = W^{[\\ell]} a^{[\\ell - 1]} + b^{[\\ell]}$\n",
    "- $X = a^{[0]}, \\hat{y} = a^{[L]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Propagation\n",
    "![](images/propgate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameters:\n",
    "![](images/params.png)\n",
    "Applied deep learning is a very empirical process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
