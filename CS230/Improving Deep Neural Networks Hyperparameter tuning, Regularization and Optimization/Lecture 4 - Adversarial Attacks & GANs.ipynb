{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Adversarial Examples\n",
    "Motivation: several machine learning models, including state-of-the-art neural networks, are vulnerable to adversarial examples.\n",
    "\n",
    "### Attacking a network with adversarial examples\n",
    "Goal: Given a network pretrained on ImageNet, find an input image that is not a iguana but will be classified as an iguana\n",
    "![](images/41.png)\n",
    "---\n",
    "And note that the ouput may not look like a iguana since the loss function is very unconstrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/42.png?'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Given a network pretrained on ImageNet, find an input image that is __a cat__ but will be calssify as an iguana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is like this:\n",
    "![](images/44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defenses against adversarial examples\n",
    "Type of attacks:\n",
    "- Non-targeted attacks: Just want to find an adversarial example that could fool the model\n",
    "- Targeted attacks: The one can fool the model is actually in one of a specific class\n",
    "\n",
    "Knowledge of the attacker:\n",
    "- White box: Known all params and architecture, just back-prob them and update the image\n",
    "- Black box: Numerical gradient; If the model is not reachable, just train it by your own model since generally, these adversarial examples are highly transferable.\n",
    "\n",
    "##### Solution\n",
    "1. SafetyNet, is fake like forged or real\n",
    "2. Train on correctly labelled adversarial examples\n",
    "    - $\\times$ overfit, costly\n",
    "3. Adversarial training:\n",
    "    - $L_{new} = L(W, b, x, y) + \\lambda L(W, b, x_{adv}, y)$\n",
    "    - For every iteration of out gradient descent, we're going to iterate enough to forge an adversarial example, and find the loss of second term, and back prob the overall loss\n",
    "4. Adversarial logit paring:\n",
    "    - $L_{new} = L(W, b, x, y) + \\lambda ||f(x;W, b) - f(x_{adv}; W, b)||_2^2$   \n",
    "    \n",
    "The existance of adversarial examples is not due to the high non-linearily and overfitting but because of the linear parts of our networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the linear regression. We want to find $x^*$ such that is similar to $x$ but output a different label. Assume we have $\\frac{\\partial L}{\\partial x} = W $. Let $x^* = x + \\epsilon W$, then \n",
    "$$W^T x^* = W^T x + \\epsilon W^T W = W^T x + \\epsilon ||W||$$\n",
    "It's shown that if $W$ is large, $x^*$ will not be similar to $x$.\n",
    "But if we make $x^* = x + \\epsilon \\text{ sign}(W)$, since x will always grow in dimension, the impact on $\\hat{y}$ of $+ \\epsilon \\text{ sign}(W)$ increase.\n",
    "\n",
    "Note that the tendency is trying to linarize all the behaviors of these neural networks, like ReLU, or Xavier initialization. Networks that have high gradients and operating in the linear regime are more vulnerable to adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast Gradient Sign Method\n",
    "$$x^* = x + \\epsilon \\text{ sign} (\\nabla_x g(w, x, y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose((1, 2, 0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs\n",
    "1. Motivation: endowing computers with an understanding of our world\n",
    "2. Goal: collect a lot of data, use it to train a model to generate similar data from scratch\n",
    "3. Intuition: Number of parameters of the model << amount of data\n",
    "![](images/45.png?)\n",
    "![](images/46.png?)\n",
    "![](images/47.png?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tricks\n",
    "#### 1.\n",
    "The instructor mentioned that when train G, just flip the sign of gradient; and GAN is difficult to train, some tricks may needed.\n",
    "![](images/48.png)\n",
    "We want $D(G(z))) = 1$, but the cost J tends to be $- \\infty$ which is called \"saturating\", so using some trick such that\n",
    "$$\\min \\log (1 - D(G(z))) \\equiv \\max \\log (D(G(z))) \\equiv \\min - \\log(D(G(z)))$$\n",
    "then we will have a non-saturating cost. So the new $J^{(G)}$ is\n",
    "$$J^{(G)} = - \\frac{1}{m_{\\text{gen}}} \\log (D(G(z)))$$\n",
    "![](images/410.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\n",
    "D is usually easier to train than G, the performance of D is an upper bound to what G can archieve. So we usually train D more time than G, so usually the training process is:\n",
    "```python\n",
    "for num_iterations:\n",
    "    for k iterations:\n",
    "        update D\n",
    "    update G\n",
    "```\n",
    "We may also use different learning rate for D and G to train faster the distriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@virtual batchnorm???\n",
    "@one-sided label smoothing??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/411.png)\n",
    "It's shown that linear operations in the latent space of codes have impact directly on the image spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert horses to zebras on images and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data\n",
    "    - Unpaired images of horses and zebras\n",
    "2. Architecture:\n",
    "    ![](images/cycan.png)\n",
    "3. Loss\n",
    "    ![](images/412.png)\n",
    "4. Evaluate @\n",
    "    ![](images/413.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
