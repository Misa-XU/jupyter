{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Courses related to general \"AI\":\n",
    "CS221\n",
    "CS229A\n",
    "CS231\n",
    "CS228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other portfolios in \"AI\"\n",
    "- @ Base Error Rate\n",
    "- @ Probabilistic graphical model\n",
    "- @ Planing Algorithm\n",
    "- @ Searching Algorithm\n",
    "- @ Game Theory\n",
    "- @ Knowledge Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course will also teach us how to build a machine learning system efficiently, and desicion stuff, like, should we collect more data. __More data pretty much never hurts__, but sometimes is not worthful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Label__ is important since it tells us the correspondence between the output layer and the context of the problem.\n",
    "\n",
    "Some strategies to label the data:\n",
    "1. Integer that each corresponds to a certain animal\n",
    "2. One-hot Vector\n",
    "    - Cons: \n",
    "        1. Imbalance. Since there's too many 0 and only one 1, the network tends to output zero at the neurons in the last layer.\n",
    "        2. Fails when there are multiple animals in the image\n",
    "3. Preferable: \n",
    "__Multi-hot vector__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only output one animal per image, the __loss function__ could be __softmax__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to solve a deep learning problem\n",
    "- Analyze a problem from a deep learning approach\n",
    "- Choose an architecture\n",
    "- Choose a loss and a training strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Day'n'Night classification\n",
    "Goal: Given an image, classify as taken \"during the day\" or \"during the night\"\n",
    "#### 1. Data:\n",
    "    - Labeled images captured during the day and night\n",
    "1. How many images:\n",
    "    - $\\times$ A number that's similar to the number of parameters (But we cannot know exactly what our network will look like)\n",
    "    - Depend on the computation resources\n",
    "    - Specify the problem:\n",
    "        1. Do we need to classify the complicated images like taken from indoor or the __edges__ like dawn, twilight\n",
    "    - Get reference from other network\n",
    "        - Suppose 10k images is enough to solve the cat recognition task, is it as complicate as this problem?\n",
    "2. How to split:\n",
    "    - Actually it depens on how large scale of data you need to analyze the performance. In this example, maybe 2k is enough, so if the amount of images is 10k, we apply 80-20; If having 1000k images, may apply 98-2\n",
    "3. Bias:\n",
    "    - Need correct balance between different classes.\n",
    "    \n",
    "#### 2. Input:\n",
    "    - A Pixel image\n",
    "1. Resolution:\n",
    "    - Should be as low as you can, but how to get this number:\n",
    "        - Compare to human performance. Here 64 * 64 * 3 is enough\n",
    "\n",
    "#### 3. Output: Label\n",
    "- Last activation function: Sigmoid\n",
    "\n",
    "#### 4. Architecture: Shallow NN\n",
    "\n",
    "#### 5. Loss Function: Cross-Entrophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Verification\n",
    "Goal: A school want to use Fae verification for validating student IDs in facilities\n",
    "#### 1. Data\n",
    "- Picture of every student labelled with their name\n",
    "\n",
    "#### 2. Input\n",
    "    - The person standing in front of the camera\n",
    "1. Resolution:\n",
    "    - Should be larger because more details need to be detected 412 * 412 * 3\n",
    "    - Color is important since the \"settings\" like brightness, illuminance will defer\n",
    "    \n",
    "#### 3. Output: 0 or 1\n",
    "#### 4. Architecture:\n",
    "1. $\\times$ Hash or Distance:\n",
    "    - Fails since our background or make-up may defer\n",
    "2. $\\surd$ Encoding them and then calculate the distance\n",
    "\n",
    "#### 5. Training\n",
    "1. We may use public face datasets\n",
    "2. $\\times$ One-hot vector is not applicable since we may need to update the network every year when adding people to the database\n",
    "3. We want similar encoding from same people and different encoding from different people\n",
    "\n",
    "#### 6. Loss: Triples\n",
    "$$L = ||Enc(A) - Enc(P)||_2^2 - ||Enc(A) - Enc(N)||_2^2 + \\alpha$$ \n",
    "Here $\\alpha$ is called __margin__ and it pushes the network to learn someting meaningful to stabilize itself on zeros.\n",
    "![](images/22.png)\n",
    "@Error analysis \n",
    "\n",
    "@FaceNet\n",
    "\n",
    "#### 7. Face Recognition\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "#### 8. Face Clustering\n",
    "- K means\n",
    "    - X-means to define K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Art Generation\n",
    "#### 1. Data\n",
    "Let's say we have any data\n",
    "#### 2. Input\n",
    "- Content image\n",
    "- Style image\n",
    "\n",
    "#### 3. Output\n",
    "- Generated image\n",
    "\n",
    "#### 4. Architecture\n",
    "1. A existing model that understand image very well\n",
    "    - When forward propagates, we get the information about its content\n",
    "2. Use gram matrix to extract style S\n",
    "\n",
    "#### 5. Loss\n",
    "$$L = ||Style_s - Style_G||_2^2 + ||Content_c - Content_G||_2^2$$\n",
    "\n",
    "#### 6. Training\n",
    "We dont train the parameters of the neural network; Instead, we train the image!!\n",
    "![](images/23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Word Detection\n",
    "Goal: Given a 10 sec audio speech, detect the word \"activate\"\n",
    "\n",
    "#### 1. Data\n",
    "A bunch of 10s audio clips\n",
    "#### 2. Input\n",
    "![](images/24.png)\n",
    "1. Resolution: Compare to human performance\n",
    "2. Label\n",
    "    - $\\times$ 0 or 1 (need tons of data)\n",
    "    - ![](images/25.png) $\\times$ Here 1 should be after the word was said (highly unbalanced)\n",
    "    - $000...1111111..000$ Add some 1's after the word appeared to make it less \"unbalanced\"\n",
    "    \n",
    "#### 3. Last Activation: Sequential sigmoid\n",
    "#### 4. Architecture: \n",
    "1. RNN\n",
    "2. Triplet loss algorithm\n",
    "\n",
    "#### 5. Loss\n",
    "$$L = - (y \\log(\\hat{y}) + (1 - y) \\log (1 - \\hat{y}))$$\n",
    "#### 6. Strategic data collection/ labelling process\n",
    "Programmatic generation of samples and automated labeling\n",
    "![](images/26.png)\n",
    "@spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO\n",
    "Loss function\n",
    "$$\\begin{align*}\n",
    "L = & \\lambda_{coord} \\sum_i^{S^2} \\sum_j^{B} \\mathbb{1}^{\\text{obj}}_{ij} [(x_i - \\hat{x}_i)^2 - (y_i - \\hat{y}_i)^2] \\\\\n",
    "& + \\lambda_{coord} \\sum_i^{S^2} \\sum_j^{B} \\mathbb{1}^{\\text{obj}}_{ij} [(w_i - \\hat{w}_i)^2 - (h_i - \\hat{h}_i)^2] \\\\\n",
    "& + \\sum_i^{S^2} \\sum_j^{B} \\mathbb{1}^{\\text{obj}}_{ij} (C_i - \\hat{C}_i)^2 \\\\\n",
    "& + \\lambda\\sum_i^{S^2} \\sum_j^{B} \\mathbb{1}^{\\text{obj}}_{ij} (C_i - \\hat{C}_i)^2 \\\\\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
