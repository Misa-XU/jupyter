{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set \\#1 : Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "--- \n",
    "#### 1.1 Consider the average empirical loss (the risk) for logistic regression:\n",
    "   $$J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^m\\,log(1 + e^{-y^{(i)}\\theta^{T}x^{(i)}}) = - \\frac{1}{m} \\sum_{i = 1}^m\\,log(h_\\theta(y^{(i)}\\theta^Tx^{(i)}))$$\n",
    "   where $y^{(i)}\\,\\in\\, \\{-1, 1\\},\\, h_\\theta(x) = g(\\theta^Tx)\\, and \\,g(z) = 1\\,/\\,(1+e^{-z}).$ Find the Hessian $H$ of this function, and show that for any vector $z$, it holds true that $$z^THz\\geq 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's calculate the first derivative of $J(\\theta)$ first. Using the chain rule, we could have\n",
    "$$\\frac{\\partial}{\\partial\\theta}J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial}{\\partial\\theta}log(h_\\theta(y^{(i)}x^{(i)})) $$\n",
    "$$ = - \\frac{1}{m} \\sum_{i = 1}^m \\frac {1} {h_\\theta(y^{(i)}x^{(i)})} \\, \\frac{\\partial}{\\partial\\theta} h_\\theta(y^{(i)}x^{(i)})$$\n",
    "Since we've already known that $g'(z) = g(z)(1-g(z))$, therefore\n",
    "$$ = - \\frac{1}{m} \\sum_{i = 1}^m \\frac {1} {h_\\theta(y^{(i)}x^{(i)})}\\, h_\\theta(y^{(i)}x^{(i)})\\, (1 - h_\\theta(y^{(i)}x^{(i)})) \\frac{\\partial}{\\partial\\theta} y^{(i)}\\theta^Tx^{(i)} $$\n",
    "$$ = - \\frac{1}{m} \\sum_{i = 1}^m  (1 - h_\\theta(y^{(i)}x^{(i)}))y^{(i)}x^{(i)} $$\n",
    "\n",
    "Then let's find the Hessian matrix\n",
    "$$H = \\frac {\\partial^2}{\\partial \\theta^2} J(\\theta) = - \\frac{1}{m} \\sum_{i = 1}^m  \\frac {\\partial}{\\partial \\theta} (1 - h_\\theta(y^{(i)}x^{(i)}))y^{(i)}x^{(i)}$$\n",
    "$$ = - \\frac{1}{m} \\sum_{i = 1}^m  \\frac {\\partial}{\\partial \\theta} y^{(i)}x^{(i)} + \\frac{1}{m} \\sum_{i = 1}^m  \\frac {\\partial}{\\partial \\theta}y^{(i)}x^{(i)}   h_\\theta(y^{(i)}x^{(i)}))$$\n",
    "\n",
    "$$ = 0 + \\frac{1}{m} \\sum_{i = 1}^m  h_\\theta(y^{(i)}x^{(i)})\\, (1 - h_\\theta(y^{(i)}x^{(i)})) {y^{(i)}}^2 \\frac{\\partial}{\\partial\\theta} x^{(i)} y^{(i)}(\\theta^Tx^{(i)}) $$\n",
    "Since ${y^2 = 1}$, Hessian matrix shoule be\n",
    "$$ = \\frac{1}{m} \\sum_{i = 1}^m  {y^{(i)}}^2{x^{(i)}}{x^{(i)}}^T h_\\theta(y^{(i)}x^{(i)})\\, (1 - h_\\theta(y^{(i)}x^{(i)}))$$\n",
    "It's obvious that  $\\frac{1}{m} {y^{(i)}}^2 h_\\theta(t)\\, (1 - h_\\theta(t)) > 0$ for any $t$, ${x^{(i)}}^2{x^{(i)}}^2$ is outer product; So $H \\succeq 0$. It's _positive semidefinite_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 We have provided two data files:\n",
    "   - [dataset1](../blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
