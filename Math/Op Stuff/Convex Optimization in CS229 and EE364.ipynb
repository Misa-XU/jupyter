{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Sets\n",
    "A set C is said to be convex if for any $x, y \\in C$, $\\theta \\in \\mathbb R$ and $0 \\le \\theta \\le 1$, it satisfies\n",
    "$$\\theta x + (1 - \\theta) y \\in C$$\n",
    "Intuitively, it means that given two elements in C, draw a line segment between them, then all the points on the line are also in C. And $\\theta x + (1 - \\theta) y$ is called a __convex combination__\n",
    "\n",
    "Examples:\n",
    "1. All of $R^n$\n",
    "2. The non-negative orthant\n",
    "3. Norm balls\n",
    "4. Affine Subspaces\n",
    "5. Intersection of Convex Sets\n",
    "6. Positive semidenifite matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Functions\n",
    "A function $f: R^n \\to R$ is said to be convex if \n",
    "1. its __domain $D(f)$ is a convex set__ and \n",
    "2. for all $x, y \\in D(f)$, $\\theta \\in \\mathbb R$ and $0 \\le \\theta \\le 1$, $f(\\theta x + (1 - \\theta) y) < \\theta f(x) + (1 - \\theta) f(y)$\n",
    "\n",
    "Note that we say $f$ is __concave__ if $-f$ is convex. And we say $f$ is __strictly convex__ is the inequality strictly holds.\n",
    "\n",
    "#### Properties of Convex Functions\n",
    "1. Suppose a $f$ is differentiable, then $f$ is convex if and only if $D(f)$ is a convex set and $$f(y) \\ge f(x) + (y - x)^T\\nabla_x f(x), \\forall x \\in D(f)$$\n",
    "2. Suppose $f$ is twice differentiable, then $f$ is convex if and only if $D(f)$ is a convex set and the Hessian is positive semidefinite\n",
    "$$\\nabla^2 f(x) \\succeq 0, \\forall x \\in D(f)$$\n",
    "3. __Jenson's Inequality__. let $\\theta$ be a probability representation, then \n",
    "$$f(\\sum_{i = 0}^k \\theta_if(x_i)) = f(\\int p(x) f(x)) \\le \\int p(x) f(x), \\forall x \\text{ and } \\int p(x) dx = 1$$\n",
    "Alternatively, it means that $f(E(x)) \\le E(f(x))$\n",
    "4. __$\\alpha$ sublevel set__. Given a convex function $f$, the $\\alpha$-sublevel set is\n",
    "$$\\{x \\in D(f) : f(x) \\le \\alpha\\}$$\n",
    "Then it is also a convex set because\n",
    "$$f(\\theta x + (1 - \\theta) y) \\le \\theta f(x) + (1 - \\theta) f(y) \\le \\theta \\alpha + (1 - \\theta)\\alpha = \\alpha, \\forall x, y \\,s.t. f(x) \\le \\alpha,\\, f(y) \\le \\alpha$$\n",
    "\n",
    "Note that the convex function is not necessarily be differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Optimization Problems\n",
    "A problem is said to be a convex optimization problems if it could be written as\n",
    "<center>\n",
    "minimize $f(x)\\quad\\quad\\quad\\quad\\quad\\quad\\quad$ <br>\n",
    "    subject to $\\begin{align*}g_i(x) & \\le 0, \\, i = 1, ..., m \\\\\n",
    "    h_i(x) & = 0, \\, i = 1, ..., p\\end{align*}$\n",
    "</center>\n",
    "Where $f$ is convex function, $g_i$ are convex function, $h_i$ are affine functions and $x$ is the optimization variable. The optimal value is denoted $p^*$ \n",
    "\n",
    "Note that \n",
    "1. Given a convex function $g_i$, we create a 0-sublevel set which construct a convex set. And then the intersection of convex sets is also convex. And the equality can be treated as the intersection of $h \\le 0$ and $h \\ge 0$, so it must be affine.\n",
    "2. For a convex optimization problem, all locally optimal points are galobally optimal.\n",
    "3. It's canonical to make op problem as a problem to _minimize_ something.\n",
    "4. p* could be $+\\infty$ or $-\\infty$ when the problem is either infeasible or unbounded below\n",
    "\n",
    "#### Special cases of Convex Problems\n",
    "Although most of the convex problems is not easy to solve, but some others are well-developed. As long as you convert a convex problems into one of the following type, you could solve it easily.\n",
    "1. Linear Programming\n",
    "<center>\n",
    "minimize $c^Tx + d\\quad$ <br>\n",
    "    subject to $\\begin{align*} G(x) & \\preceq h, \\\\\n",
    "    A(x) & = b \\end{align*}$\n",
    "</center>\n",
    "Where $x, c \\in \\mathbb R$, $d \\in \\mathbb R$, $G \\in \\mathbb R^{m \\times n}$, $h \\in \\mathbb R^m$, $A \\in \\mathbb R^{p \\times n}$, $b \\in \\mathbb R^p$, $\\preceq$ denotes elementwise inequality\n",
    "\n",
    "1. Quadratic Programming\n",
    "<center>\n",
    "minimize $\\frac{1}{2}x^T P x + c^Tx + d$ <br>\n",
    "    subject to $ \\begin{align*} G(x) & \\preceq h, \\quad\\quad \\\\\n",
    "    A(x) & = b \\end{align*}$\n",
    "</center>\n",
    "Where $x, c \\in \\mathbb R$, $d \\in \\mathbb R$, $G \\in \\mathbb R^{m \\times n}$, $h \\in \\mathbb R^m$, $A \\in \\mathbb R^{p \\times n}$, $b \\in \\mathbb R^p$, $P \\in \\mathbb S^n_+$\n",
    "\n",
    "1. Quadratically Constrained Quadratic Programming\n",
    "<center>\n",
    "minimize $ \\frac{1}{2}x^T P x + c^Tx + d \\quad$ <br> \n",
    "    subject to $\\begin{align*} & \\frac{1}{2}x^T Q_i x + r_i^T x + s_i \\le 0, \\\\\n",
    "    & A(x)  = b \\end{align*}$\n",
    "</center>\n",
    "Where $x, cï¼Œ r_i \\in \\mathbb R$, $d, s_i \\in \\mathbb R$, $A \\in \\mathbb R^{p \\times n}$, $b \\in \\mathbb R^p$, $Q_i, P \\in \\mathbb S^n_+$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal and Dual\n",
    "#### Primal optimization problem\n",
    "$$\\begin{align*}\n",
    "\\underset{w}{\\mathrm{min }} & f(w) \\\\\n",
    "\\text{s.t. } & g_i(w) \\le 0, i = 1, ..., k \\\\\n",
    "& h_i(w) = 0, i = 1, ..., l\n",
    "\\end{align*}$$\n",
    "\n",
    "To solve it, we start by defining the generalized Lagrangian \n",
    "$$L(w, \\alpha, \\beta) = f(w) + \\sum_{i = 1}^k \\alpha_i g_i(w) + \\sum_{i = 1}^l \\beta_i h_i(w)$$\n",
    "Where $\\alpha_i$ and $\\beta_i$ are called the __dual variables__ and $w$ are the __primal variables__. Consider the adversial case that we want to manipulate $w$ to minimize $L$ and our opponent manipulates $\\alpha, \\beta$ to maximize $L$, then consider the quantity $$\\theta_P(w) = \\underset{\\alpha, \\beta: \\alpha_i \\ge 0}{\\max} L(w, \\alpha, \\beta)$$ Minimizing $\\theta_P(w)$ is equivalent to our original convex op problem.\n",
    "We could find that\n",
    "$$\\begin{equation*} \n",
    "\\theta_P(w) = \\begin{cases}\n",
    "f(w) & \\text{if all constraints be satisified} \\\\\n",
    "\\infty & \\text{otherwise}\n",
    "\\end{cases}\\end{equation*}$$\n",
    "Intuitively, $\\theta_P(x)$ behaves like an \"unconstrained\" version of the orignal problem which the infeasible region of f is \"carved away\" by forcing $\\theta_P(w) = \\infty$ for any infeasible w.\n",
    "\n",
    "Let $p^* = \\underset{w}{\\min} \\theta_P(w)$, it's called the value of primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual  problem\n",
    "We define its __Dual Problem__ as\n",
    "$$d^* = \\underset{\\alpha, \\beta: \\alpha_i \\ge 0}{max} \\underset{w}{\\min} L(w, \\alpha, \\beta) = \\underset{\\alpha, \\beta: \\alpha_i \\ge 0}{max} \\theta_D(\\alpha, \\beta)$$\n",
    "Usually, it's easier to solve the dual problem.\n",
    "\n",
    "##### Weak Duality\n",
    "$$\\underset{\\alpha, \\beta: \\alpha_i \\ge 0}{\\max} \\underset{w}{\\min} L(x, \\alpha, \\beta) = d^* \\le p^* = \\underset{w}{\\min} \\underset{\\alpha, \\beta: \\alpha_i \\ge 0}{\\max} L(x, \\alpha, \\beta)$$\n",
    "##### Strong Duality\n",
    "Consider a convex optimization problem, whose corresponding primal and dual problems are P and D. If there exists a primal feasible x that stricly holds all inequality, then $p^* = d^*$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
