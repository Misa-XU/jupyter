{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset {$(x^{(i)}, y^{(i)}); i = 1, ..., m$} consisting of m independent examples, where $x^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\{-1, 1\\}$. The joint distribution of (x, y) is modeled according to:\n",
    "$$p(y) = \\begin{cases}\n",
    "               \\phi, & \\text{if}\\ y = 1 \\\\\n",
    "               1 - \\phi, &\\text{if}\\ y = - 1 \n",
    "           \\end{cases}$$\n",
    "$$p(x|y = -1) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_{-1})^T \\Sigma^{-1} (x - \\mu_{-1}))$$\n",
    "and\n",
    "$$p(x|y = 1) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_{1})^T \\Sigma^{-1} (x - \\mu_{1}))$$\n",
    "\n",
    "Note that the two distribution have the same covariance matrix $\\Sigma$, the other parameters are $\\phi$, $\\mu_{-1}$, $\\mu_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1.  Show that the posterios distribution of the label at x taks that form of a logistic function, that can be written as\n",
    "$$p(y|x;\\phi, \\Sigma, \\mu_{-1}, \\mu) = \\frac{1}{1 + exp(-y(\\theta^Tx + \\theta_0))}$$ where $\\theta \\in \\mathbb{R}^n$ and the bias term $\\theta_0 \\in \\mathbb{R}$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A wrong answer using GLM\n",
    "$$E(Y) = \\mu = 1 \\cdot \\phi + (-1) \\cdot (1 - \\phi) = 2\\phi - 1$$\n",
    "Then similar with the formula in lecture notes, we need to find two functions $c(x)$ and $d(x) s.t.$\n",
    "$$c(1) = 1, c(-1) = 0, d(1) = 0, d(-1) = 1$$\n",
    "Then $p(y|\\phi)$ could be written as\n",
    "$$p (y \\vert \\phi) = \\phi^{\\frac{1}{2}y + \\frac{1}{2}} (1 - \\phi)^{-\\frac{1}{2}y + \\frac{1}{2}}$$\n",
    "Then\n",
    "$$log(p | \\phi) = \\frac{1}{2}((y + 1)log(\\phi) + (-y + 1)log(1 - \\phi))$$\n",
    "$$= \\frac{1}{2}(ylog(\\frac{\\phi}{1 - \\phi}) + log(\\phi(1 - \\phi)))$$\n",
    "Then we find the form of exponential family, where\n",
    "$$\\eta = \\theta^Tx = log(\\frac{\\phi}{1 - \\phi})$$\n",
    "$$b(\\theta) = log(\\phi(1 - \\phi))$$\n",
    "At the beginning we show that $\\mu = w(\\phi)$, then we could use find $\\eta$ given $\\phi$, and the link function $g(\\mu) = \\eta$ can be written as \n",
    "$$ gw(\\phi) = g(w(\\phi)) = \\eta$$\n",
    "So $$\\phi = \\frac{1}{1 + e ^ {-\\theta^T\\mathbf{x}}}$$\n",
    "#### I think it's shown that GLM can solve $y \\to \\{-1, 1\\}$ but it's not the case. In Bayes method dataset is expected to be splitted by y. So the y should be the part of the hypothesis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Bayes Theorem, we have\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "p(y = 1 | x) & = p(y = 1) \\frac{p(x|y = 1)}{p(x)} \\\\\n",
    "& = \\frac{p(y = 1) p(x| y = 1)}{p(y = 1) p(x|y = 1) + p(y = -1)p(x|y = -1)} \\\\\n",
    "& = \\frac{1}{1 + \\frac{p(y = -1)}{p(y = 1)}\\frac{p(x|y = -1)}{p(x|y = 1)}} \\\\\n",
    "& = \\frac{1}{1 + e^{1 * log \\frac{p(y = -1)}{p(y = 1)}\\frac{p(x|y = -1)}{p(x|y = 1)}}}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "Similarly, we have\n",
    "$$p(y = 1 | x) = \\frac{1}{1 + e^{-1 * log \\frac{p(y = -1)}{p(y = 1)}\\frac{p(x|y = -1)}{p(x|y = 1)}}}$$\n",
    "So now we could say\n",
    "$$p(y|x) = \\frac{1}{1 + e^{yh(x)}}$$\n",
    "Next, we need to prove that $log \\frac{p(y = -1)}{p(y = 1)}\\frac{p(x|y = -1)}{p(x|y = 1)}$ can be rewritten as a linear relationship $s.t.\\, h(x) = \\theta^T x$.\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "log \\frac{p(y = -1)}{p(y = 1)}\\frac{p(x|y = -1)}{p(x|y = 1)} & = \n",
    "log \\frac{1 - \\phi}{\\phi} e^{-\\frac{1}{2} ((x - \\mu_{-1})^T \\Sigma^{-1} (x - \\mu_{-1}) - ( x - \\mu_1)^T \\Sigma^{-1} ( x - \\mu_1) )} \\\\\n",
    "& = log \\frac{1 - \\phi}{\\phi} -\\frac{1}{2} ((x - \\mu_{-1})^T \\Sigma^{-1} (x - \\mu_{-1}) - ( x - \\mu_1)^T \\Sigma^{-1} ( x - \\mu_1) )\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "Since they share the same covariance, in the polynomials$(x - \\mu_{-1})^T \\Sigma^{-1} (x - \\mu_{-1})$ and $(x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1)$, for each term contains $x_ix_k$, the coefficient is identical. So after subtraction, only the terms contain a $x_i$ and a $\\mu$ could be reserved. So it's actually a linear combination of $x_i$ with different paramters $\\theta_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Given m samples, the log likelihood if the data is:\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\phi,\\mu_{-1},\\mu_1, \\Sigma) & = log \\prod_{i = 1}^m p(x^{(i)}, y^{(i)}; \\phi,\\mu_{-1},\\mu_1,\\Sigma) \\\\\n",
    "& = log \\prod_{i = 1}^m p(x^{(i)}| y^{(i)}; \\mu_{-1},\\mu_1,\\Sigma) p(y^{(i)};\\phi)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "Show that the maximum likelihood estimators of $\\phi,\\,\\mu_{-1},\\,\\mu_1, \\Sigma$ are\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi & = \\frac{1}{m} \\sum_{i = 1}^m 1\\{y^{(i)} = 1\\} \\\\\n",
    "\\mu_{-1} & = \\frac{\\sum_{i = 1}^m 1\\{y^{(i)} = - 1\\} x^{(i)}}{\\sum_{i = 1}^m 1\\{y^{(i)} = - 1\\}} \\\\\n",
    "\\mu_{1} & = \\frac{\\sum_{i = 1}^m 1\\{y^{(i)} =  1\\} x^{(i)}}{\\sum_{i = 1}^m 1\\{y^{(i)} =  1\\}} \\\\\n",
    "\\Sigma & = \\frac{1}{m} \\sum_{i = 1}^m (x^{(i)} - u_{y^{(i)}})(x^{(i)} - u_{y^{(i)}})^T\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each proof, the common derivation we would use is\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta} l(\\phi,\\mu_{-1},\\mu_1, \\Sigma)  = \\frac{\\partial}{\\partial \\theta} \\Big( \\sum_{i = 1}^m log \\,p(x^{(i)}| y^{(i)}) + \\sum_{i = 1}^m log\\, p(y^{(i)};\\phi) \\Big)\\\\\n",
    "$$\n",
    "Assume $s = \\#\\{y = 1\\}$, $t = \\#\\{y = -1\\}$\n",
    "#### a\n",
    "Let's find $\\hat{\\phi}$ first \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\phi} l(\\phi,\\mu_{-1},\\mu_1, \\Sigma) \n",
    "& = \\frac{\\partial}{\\partial \\phi} \\sum_{i = 1}^m log\\, p(y^{(i)};\\phi) \\\\\n",
    "& = \\frac{\\partial}{\\partial \\phi} \\Big( s log(\\phi) + t log(1 - \\phi) \\Big) \\\\\n",
    "& = \\frac{s}{\\phi} - \\frac{t}{1 - \\phi}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "Let $\\frac{\\partial}{\\partial \\phi} l(\\theta) = 0$, then\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{s}{\\hat{\\phi}} - \\frac{t}{1 - \\hat{\\phi}} & = 0 \\\\\n",
    "s - s\\hat{\\phi} & = t\\hat{\\phi} \\\\\n",
    "\\hat{\\phi} & = \\frac{s}{s + t}\n",
    "\\end{align*}\n",
    "$$\n",
    "Which means $\\hat{\\phi} = \\frac{1}{m} \\sum_{i = 1}^m 1\\{y^{(i)} = 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\mu_1} l(\\phi,\\mu_{-1},\\mu_1, \\Sigma) & = \\frac{\\partial}{\\partial \\mu_1} \\Big( \\sum_{i = 1}^s log \\,p(x^{(i)}| y^{(i)} = 1) +  \\sum_{i = 1}^t log \\,p(x^{(i)}| y^{(i)} = - 1) + \\sum_{i = 1}^m log\\, p(y^{(i)};\\phi) \\Big)\\\\\n",
    "& = \\frac{\\partial}{\\partial \\mu_1} \\sum_{i = 1}^s log \\,p(x^{(i)}| y^{(i)} = 1) \\\\\n",
    "& = \\frac{\\partial}{\\partial \\mu_1} \\sum_{i = 1}^s log \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_{1}))\\\\\n",
    "& = -\\frac{1}{2} \\sum_{i = 1}^s \\frac{\\partial}{\\partial \\mu_1} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) \\\\\n",
    "& = \\frac{1}{2} \\sum_{i = 1}^s \\Sigma^{-1} (x - \\mu_1) = 0\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "Since covariance matrix is positive definite, as a linear combination, it could be rewritten as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\sum_{i = 1}^s \\Sigma^{-1} (x - \\hat{\\mu_1}) & = \\Sigma^{-1} \\sum_{i = 1}^s  (x - \\hat{\\mu_1}) = 0 \\\\\n",
    "\\sum_{i = 1}^s  (x - \\hat{\\mu_1}) & = 0 \\\\\n",
    "s\\hat{\\mu_1} & = \\sum_{i = 1}^s x \\\\\n",
    "\\hat{\\mu_1} & = \\frac{\\sum_{i = 1}^s x}{s}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "which means\n",
    "$$\\mu_{1} = \\frac{\\sum_{i = 1}^m 1\\{y^{(i)} =  1\\} x^{(i)}}{\\sum_{i = 1}^m 1\\{y^{(i)} =  1\\}}$$\n",
    "Similarly, we have\n",
    "$$\\mu_{-1} = \\frac{\\sum_{i = 1}^m 1\\{y^{(i)} = - 1\\} x^{(i)}}{\\sum_{i = 1}^m 1\\{y^{(i)} = - 1\\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### c\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\Sigma^{-1}} l(\\phi,\\mu_{-1},\\mu_1, \\Sigma) \n",
    "& = \\frac{\\partial}{\\partial \\Sigma^{-1}} \\Big( \\sum_{i = 1}^s log \\,p(x^{(i)}| y^{(i)} = 1) +  \\sum_{i = 1}^t log \\,p(x^{(i)}| y^{(i)} = - 1) + \\sum_{i = 1}^m log\\, p(y^{(i)};\\phi) \\Big)\\\\\n",
    "& = \\frac{\\partial}{\\partial \\Sigma^{-1}} \\Big( \\sum_{i = 1}^s log \\,p(x^{(i)}| y^{(i)} = 1) + \\sum_{i = 1}^t log \\,p(x^{(i)}| y^{(i)} = - 1) \\Big)\\\\\n",
    "& = \\frac{\\partial}{\\partial \\Sigma^{-1}} \\Big( \\sum_{i = 1}^s log \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_{1})) + \\sum_{i = 1}^t log \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_{1})) \\Big)\\\\\n",
    "& = -\\frac{1}{2} \\Big( \\sum_{i = 1}^s \\frac{\\partial}{\\partial \\Sigma^{-1}} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) + \\sum_{i = 1}^t \\frac{\\partial}{\\partial \\Sigma^{-1}} (x - \\mu_{-1})^T \\Sigma^{-1} (x - \\mu_{-1}) \\Big) + \\frac{m}{2} \\frac{\\partial}{\\partial \\Sigma^{-1}} log \\frac{|\\Sigma^{-1}|}{2\\pi}\\\\\n",
    "& = - \\frac{1}{2} \\Big(  \\sum_{i = 1}^s (x - \\mu_1)(x - \\mu_1)^T + \\sum_{i = 1}^t (x - \\mu_{-1})(x - \\mu_{-1})^T \\Big) + \\frac{m}{2} \\Sigma= 0 \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "So we have\n",
    "$$\\Sigma = \\frac{1}{m} \\sum_{i = 1}^m (x^{(i)} - u_{y^{(i)}})(x^{(i)} - u_{y^{(i)}})^T$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
