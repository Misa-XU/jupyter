{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset {$(x^{(i)}, y^{(i)}); i = 1, ..., m$} consisting of m independent examples, where $x^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\{-1, 1\\}$. The joint distribution of (x, y) is modeled according to:\n",
    "$$p(y) = \\begin{cases}\n",
    "               \\phi, & \\text{if}\\ y = 1 \\\\\n",
    "               1 - \\phi, &\\text{if}\\ y = - 1 \n",
    "           \\end{cases}$$\n",
    "$$p(x|y = -1) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_{-1})^T \\Sigma^{-1} (x - \\mu_{-1}))$$\n",
    "and\n",
    "$$p(x|y = 1) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x - \\mu_{1})^T \\Sigma^{-1} (x - \\mu_{1}))$$\n",
    "\n",
    "Note that the two distribution have the same covariance matrix $\\Sigma$, the other parameters are $\\phi$, $\\mu_{-1}$, $\\mu_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 1.  Show that the posterios distribution of the label at x taks that form of a logistic function, that can be written as\n",
    "$$p(y|x;\\phi, \\Sigma, \\mu_{-1}, \\mu) = \\frac{1}{1 + exp(-y(\\theta^Tx + \\theta_0))}$$ where $\\theta \\in \\mathbb{R}^n$ and the bias term $\\theta_0 \\in \\mathbb{R}$ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A wrong answer using GLM\n",
    "$$E(Y) = \\mu = 1 \\cdot \\phi + (-1) \\cdot (1 - \\phi) = 2\\phi - 1$$\n",
    "Then similar with the formula in lecture notes, we need to find two functions $c(x)$ and $d(x) s.t.$\n",
    "$$c(1) = 1, c(-1) = 0, d(1) = 0, d(-1) = 1$$\n",
    "Then $p(y|\\phi)$ could be written as\n",
    "$$p (y \\vert \\phi) = \\phi^{\\frac{1}{2}y + \\frac{1}{2}} (1 - \\phi)^{-\\frac{1}{2}y + \\frac{1}{2}}$$\n",
    "Then\n",
    "$$log(p | \\phi) = \\frac{1}{2}((y + 1)log(\\phi) + (-y + 1)log(1 - \\phi))$$\n",
    "$$= \\frac{1}{2}(ylog(\\frac{\\phi}{1 - \\phi}) + log(\\phi(1 - \\phi)))$$\n",
    "Then we find the form of exponential family, where\n",
    "$$\\eta = \\theta^Tx = log(\\frac{\\phi}{1 - \\phi})$$\n",
    "$$b(\\theta) = log(\\phi(1 - \\phi))$$\n",
    "At the beginning we show that $\\mu = w(\\phi)$, then we could use find $\\eta$ given $\\phi$, and the link function $g(\\mu) = \\eta$ can be written as \n",
    "$$ gw(\\phi) = g(w(\\phi)) = \\eta$$\n",
    "So $$\\phi = \\frac{1}{1 + e ^ {-\\theta^T\\mathbf{x}}}$$\n",
    "#### I think it's shown that GLM can solve $y \\to {-1, 1}$ but it's not the case. In Bayes method dataset is expected to be splitted by y. So the y should be the part of the hypothesis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
