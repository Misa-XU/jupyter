{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider using an iterative optimization algorithm (like Newton's method or gradient descent) to minimize some continuously differentiable function f(x). Suppose we initialize the algorithm at $x^{(0)} = \\mathbf{0}$. When the algorithm is run, it will produce a value of $x \\in \\mathbb{R}^n$ for each iteration $x^{(1)}, \\, x^{(2)}...$\n",
    "Now, let some non-singular square matrix $A \\in \\mathbb{R}^{n \\times n}$ be given, and define a new function $g(z) = f(Az)$. Consider using the same iterative optimization algorithm to optimize $g$ with initialization $z^{(0)} = \\mathbf{0}$.  If the values $z^{(1)}, \\, z^{(2)} ...$ produced by this method satisfy $z^{(i)} = A^{-1} x^{(i)}$ for all $i$, we say this optimization algorithm is __invariant to linear reparameterizations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show that Newton's method is invariant to linear reparameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that $\\mathbf{x}^{(i + 1)} := \\mathbf{x}^{(i)} - H_f^{-1}(x^{(i)}) \\nabla_{\\mathbf{x}}f(\\mathbf{x}^{(i)})$ and \n",
    " $\\mathbf{z}^{(i + 1)} := \\mathbf{z}^{(i)} - H_g^{-1}(z^{(i)}) \\nabla_{\\mathbf{z}}g(\\mathbf{z}^{(i)})$ \n",
    " \n",
    "Given $\\mathbf{z} = A^{-1}\\mathbf{x}$, according the chain rule, we have\n",
    "$$\\nabla_\\mathbf{z} g(\\mathbf{z}) = \\nabla_\\mathbf{z} f(A\\mathbf{z}) = \\frac{\\partial}{\\partial \\mathbf{z}} A\\mathbf{z} \\frac{\\partial}{\\partial A\\mathbf{z}} f(A\\mathbf{z}) = A^T \\nabla f(A\\mathbf{z}) = A^T \\nabla f(\\mathbf{x})$$\n",
    "Where $\\mathbf{x}^{(i)} = A\\mathbf{z}^{(i)}$\n",
    "\n",
    "And respectively, the Hessian of g by the chain rule is\n",
    "$$\\nabla^2_{\\mathbf{z}} g(\\mathbf{z}) = A^T H_f(\\mathbf{x}) A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's prove by mathematical induction.\n",
    "We already know that $\\mathbf{z}^{(0)} = \\mathbf{0} = A^{-1}\\mathbf{x}^{(0)}$. If $\\mathbf{z}^{(i)} = A^{-1}\\mathbf{x}^{(i)}$\n",
    "Then \n",
    "$$\\begin{equation}\\begin{split}\n",
    "\\mathbf{z}^{(i + 1)} & = \\mathbf{z}^{(i)} - H_g^{-1}(z^{(i)}) \\nabla_{\\mathbf{z}}g(\\mathbf{z}^{(i)}) \\\\\n",
    "& = A^{-1}\\mathbf{x}^{(i)} - A^{-1}H^{-1}_f(\\mathbf{x^{(i)}})A^{-T} A^T \\nabla f(\\mathbf{x}^{(i)}) \\\\\n",
    "& = A^{-1} \\mathbf{x}^{(i + 1)}\n",
    "\\end{split}\\end{equation}$$\n",
    "\n",
    "Then we obtain the proof. And it's invariant to linear reparameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Is gradient descent invariant to linear reparameterizations? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule of gradient descent is\n",
    "$$\\mathbf{x}^{(i + 1)} := \\mathbf{x}^{(i)} - \\nabla_{\\mathbf{x}}f(\\mathbf{x}^{(i)})$$\n",
    "Similarly, we have $\\mathbf{z}^{(0)} = A^{-1}\\mathbf{x}^{(0)}$, assume $\\mathbf{z}^{(i)} = A^{-1} \\mathbf{x}^{(i)}$\n",
    "Then\n",
    "$$\\mathbf{z}^{(i + 1)} = A^{-1} \\mathbf{x}^{(i)} - A^T\\nabla f(\\mathbf{x}^{(i)})$$ which is usually $ \\ne A^{-1} x^{(i + 1)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
