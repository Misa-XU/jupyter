{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dataset/41.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Jensen's Inequality, and the assumption that P and Q are prob distributions, we have\n",
    "$$\\sum P(x) \\log \\frac{P(x)}{Q(x)} = - \\sum P(x) \\log \\frac{Q(x)}{P(x)} = E(- \\log \\frac{Q(x)}{P(x)}) \\ge - log(E(\\frac{Q(x)}{P(x)})) = - log(1) = 0$$\n",
    "And the equation holds when $\\frac{P}{Q}$ is a constant, i. e. P = Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dataset/42.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "KL(P(X,Y)|| Q(X,Y)) & = \\sum \\sum P(x, y) \\log \\frac{P(x, y)}{Q(x, y)} \\\\\n",
    "& = \\sum \\sum p(x, y) \\Big( \\log \\frac{P(y|x)}{Q(y|x)} + \\log \\frac{P(x)}{Q(x)} \\Big) \\\\\n",
    "& = \\sum p(x) \\sum p(y| x) \\log \\frac{P(y|x)}{Q(y|x)} + \\sum p(y|x) \\sum p(x) \\log \\frac{P(x)}{Q(x)} \\\\\n",
    "& = KL(P(Y|X)||Q(Y|X)) + KL(P(X)||Q(X))\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dataset/431.jpg)\n",
    "![](dataset/432.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "KL(\\hat{P}||P_\\theta) & = \\sum_x \\hat{P}(x) \\log \\frac{\\hat{P}(x)}{P_\\theta(x)} \\\\\n",
    "& = \\sum_x \\sum_i \\frac{1}{m} 1\\{x^{(i)} = x\\} \\log \\frac{\\sum_i \\frac{1}{m} 1\\{x^{(i)} = x\\}}{P_\\theta(x)} \\\\\n",
    "& = \\sum_x \\sum_i \\frac{1}{m} 1\\{x^{(i)} = x\\} \\log \\sum_i \\frac{1}{m} 1\\{x^{(i)} = x\\} - \\sum_x \\sum_i \\frac{1}{m} 1\\{x^{(i)} = x\\} \\log {P_\\theta(x)}\n",
    "\\end{align*}$$\n",
    "The first half here is a constant, so for minimizing $KL(\\hat{P}||P_\\theta)$, we only need to minimize the second half which means to maxmize $$\\frac{1}{m} \\sum_x \\sum_i  1\\{x^{(i)} = x\\} \\log {P_\\theta(x)}$$\n",
    "We know that for a given x such that $x = a$ where a is a constant, $\\sum_{\\forall x = a} P(x) = 1\\{x^{(i)} = a\\} P(x)$, so actually\n",
    "$$\\begin{align*}\n",
    "\\frac{1}{m} \\sum_x \\sum_i  1\\{x^{(i)} = x\\} \\log {P_\\theta(x)} & = \\frac{1}{m} \\sum_{x} \\sum_{x^{(i)}} \\log P(x = x^{(i)};\\theta)\n",
    "\\end{align*}$$ which is exactly the MLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
