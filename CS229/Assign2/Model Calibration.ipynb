{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![q2](dataset/q2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show that the above property holds true for the described logistic regression model over the range $(a, b) = (0, 1)$\n",
    "Given $p(y|\\phi) = \\phi^y(1 - \\phi)^{1 - y}$\n",
    "Then we can rewrite it as a GLM\n",
    "$$\\begin{align*}\n",
    "\\log p(y|\\phi) & = y \\log \\phi + (1 - y) \\log (1 - \\phi) \\\\\n",
    "& = y \\log \\frac{\\phi}{ 1 - \\phi} + log(1 - \\phi)\n",
    "\\end{align*}$$\n",
    "It's shown that $\\mu = b'(\\theta) = \\phi$ and $\\eta = \\theta^T x = \\log \\frac{\\phi}{1 - \\phi}$, then we have\n",
    "$$\\begin{align*}\n",
    "\\theta^T x & = \\log \\frac{\\mu}{1 - \\mu}\\\\\n",
    "E(y) = \\mu & = \\frac{1}{1 + e^{- \\theta^T x}} = h_\\theta(x) = P(y = 1|x; \\theta)\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we have $$\\sum_{i \\in I_{a, b}} P(y^{(i)} = 1| x^{(i)}; \\theta) = \\sum_{i \\in I_{a, b}} E(y^{(i)})$$\n",
    "The maximum likelihood estimator of $E(y^{(i)}| x^{(i)})$ is the sample mean, so we have\n",
    "$$P(y^{(i)} = 1| x^{(i)}; \\theta) = \\frac{1\\{y^{j} = 1, x^{(j)} = x^{(i)}\\}}{|\\{j \\in I_{a, b} \\text{ and } x^{j} = x^{i}\\}|} $$\n",
    "Add them together, we gain the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) \n",
    "If we have a binary classification model that is perfectly calibrated -- that is, the property we just proved holds for any $(a, b) \\subset (0, 1)$ does it imply that the model achieves perfectr accuracy? Is the converse necessarily true?\n",
    "\n",
    "---\n",
    "1. The first half is not necessarily be true since this algorithms has a unambiguous value $0.5$. For some x such that $h_\\theta(x) = 0.5$, it will definitely fall into the region $(0.5 - \\delta, 0.5 + \\delta)$ and be calibrated properly. But the inverse is false: Given a x that makes $h_\\theta(x)$ exactly be $0.5$, we cannot ensure the value of y.\n",
    "2. The second half is also False because perfect accuracy just means that for all $\\{y|y^{(i)} = 1\\}$, $P(y^{(i)} = 1|x^{(i)}) > P(y^{(i)} = 0|x^{(i)})$, it not necessarily mean $P(y^{(i)} = 1|x^{(i)}) = \\mu$\n",
    "\n",
    "![remark](dataset/remark_q2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Discuss what effect including $L_2$ regularization in the logistic regression objective has on model calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $J_{L2}(\\theta) = J(\\theta) + \\frac{1}{2}\\lambda ||\\theta||_2^2$, so\n",
    "$$\\frac{\\partial}{\\partial \\theta}J_{L2}(\\theta) = \\lambda \\theta + X^T(Y - h_\\theta(X))$$\n",
    "To maximize likelihood, we make the partial derivative to zero, then we have\n",
    "$$X^TY + \\lambda \\theta = X^Th_\\theta(X) $$\n",
    "In the bias row where $X^T = [1,..., 1]$, we could have\n",
    "$$\\sum 1\\{y^{(i)} = 1\\} + \\lambda \\theta = \\sum h_\\theta(x)$$\n",
    "So it's not well calibrated.\n",
    "And large $\\theta$ will give a stronger assertion of $h_\\theta(x)$ whereas smaller $\\theta$ will make the prediction closer to $0.5$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
