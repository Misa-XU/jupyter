{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "def add_intercept(X_):\n",
    "    m, n = X_.shape\n",
    "    X = np.zeros((m, n + 1))\n",
    "    X[:, 0] = 1\n",
    "    X[:, 1:] = X_\n",
    "    return X\n",
    "\n",
    "def load_data(filename):\n",
    "    D = np.loadtxt(filename)\n",
    "    Y = D[:, 0]\n",
    "    X = D[:, 1:]\n",
    "    return add_intercept(X), Y\n",
    "\n",
    "def calc_grad(X, Y, theta):\n",
    "    m, n = X.shape\n",
    "    grad = np.zeros(theta.shape)\n",
    "    margins = Y * X.dot(theta)\n",
    "    probs = 1. / (1 + np.exp(margins))\n",
    "    grad = -(1./m) * (X.T.dot(probs * Y))\n",
    "    return grad\n",
    "\n",
    "def loss(X, Y, theta):\n",
    "    minus1_pred = 1. / (1 + np.exp(X.dot(theta)))\n",
    "    pos1_pred = 1. / (1 + np.exp(- X.dot(theta)))\n",
    "    y_pred = np.ones_like(minus1_pred)\n",
    "    y_pred[pos1_pred < minus1_pred] = -1\n",
    "    return y_pred, y_pred[y_pred != Y].size\n",
    "\n",
    "def logistic_regression(X, Y):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    learning_rate = 10\n",
    "    print(Y)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        prev_theta = theta\n",
    "        grad = calc_grad(X, Y, theta)\n",
    "        theta = theta  - learning_rate * (grad)\n",
    "        norm = np.linalg.norm(prev_theta - theta)\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            y_pred, los = loss(X, Y, theta)\n",
    "            print(y_pred)\n",
    "            print('Finished {0} iterations; Loss: {4}; Diff theta: {1}; theta: {2}; Grad: {3}'.format(\n",
    "                i, norm, theta, grad, los))\n",
    "        if i % 300000 == 0:\n",
    "            break\n",
    "        if norm < 1e-15:\n",
    "            print('Converged in %d iterations' % i)\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training model on data set A ====\n",
      "[-1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.  1.\n",
      "  1.  1. -1.  1.  1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.]\n",
      "[-1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "  1. -1. -1.  1.  1.  1. -1.  1.  1. -1.]\n",
      "Finished 10000 iterations; Loss: 8; Diff theta: 7.226491864936692e-07; theta: [-20.81394174  21.45250215  19.85155266]; Grad: [ 4.15154546e-08 -4.27822247e-08 -4.08456455e-08]\n",
      "[-1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "  1. -1. -1.  1.  1.  1. -1.  1.  1. -1.]\n",
      "Finished 20000 iterations; Loss: 8; Diff theta: 5.3329785269148335e-11; theta: [-20.81437785  21.45295156  19.85198173]; Grad: [ 3.06366902e-12 -3.15717192e-12 -3.01432011e-12]\n",
      "[-1.  1. -1. -1.  1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.  1.\n",
      "  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.\n",
      " -1.  1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1.  1.  1. -1.\n",
      " -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      "  1. -1. -1.  1.  1.  1. -1.  1.  1. -1.]\n",
      "Finished 30000 iterations; Loss: 8; Diff theta: 6.153480596427404e-15; theta: [-20.81437788  21.45295159  19.85198176]; Grad: [ 1.91127606e-16 -2.90264320e-16 -2.01301257e-16]\n",
      "Converged in 30395 iterations\n",
      "==== Training model on data set B ====\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print('==== Training model on data set A ====')\n",
    "    Xa, Ya = load_data('dataset/data_a.txt')\n",
    "    logistic_regression(Xa, Ya)\n",
    "\n",
    "    print('==== Training model on data set B ====')\n",
    "    #Xb, Yb = load_data('dataset/data_b.txt')\n",
    "    #logistic_regression(Xb, Yb)\n",
    "\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Stability\n",
    "Run the given logistic regression code to train two different models on A and B.\n",
    "#### a. What's the most notable difference in training on datasets A and B:\n",
    "A will be converged rapidly whereas B not; The parameters are also abnormal which is up to several hundreds.\n",
    "#### b. Find the problem.\n",
    "---\n",
    "Note that actually the algorithm used in the code is different from what we learned in the lecture. The update rule for $\\theta$ here is\n",
    "$$\\theta := \\theta + \\frac{1}{m} x^{(i)}y^{(i)}\\frac{1}{1 + \\exp^{y^{(i)} \\theta^T x^{(i)}}}$$\n",
    "The assumption is stronger than the common logistic regression so the training process is faster.\n",
    "Givin the posterior probability $$p(y|x;\\theta) = \\frac{1}{1 + \\exp(-y\\theta^T x)}$$\n",
    "Proof:\n",
    "$$\\begin{align*}\n",
    "L(\\theta) & = \\prod p(y^{(i)}|x^{(i)}; \\theta) \\\\\n",
    "\\ell (\\theta) & = - \\sum \\log(1 + \\exp(-y^{(i)} \\theta^T x^{(i)})) \\\\\n",
    "\\frac{\\partial}{\\partial} \\ell(\\theta) & = - \\sum \\frac{1}{1 + \\exp(-y^{(i)} \\theta^T x^{(i)})} \\exp(-y^{(i)} \\theta^T x^{(i)}) (-y^{(i)} x^{(i)}) \\\\\n",
    "& = \\sum \\frac{y^{(i)} x^{(i)}}{1 + \\exp(y^{(i)} \\theta^T x^{(i)})}\n",
    "\\end{align*}$$\n",
    "After calculate the loss, it's shown that in the dataset B, the prediction is already absolute correct but the grad is still large; Whereas in dataset A, error is always existing but the grad is small and could be ignored finally.\n",
    "It also shown that for a \"perfect\" dataset such as B, there are infinite number of solution for this regression problem, it can increase the likelihood by simply scaling $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. For each possible modifications, state its feasibility. \n",
    "1. Using a different constant learning rate\n",
    "    - No\n",
    "2. Decreasing the learning rate over time(e.g. scaling the initial learning rate by $1/t^2$ where $t$ is the number of iterations thus far)\n",
    "    - Yes\n",
    "3. Adding a regularization term $||\\theta||_2^2$ to the loss function.\n",
    "    - Yes\n",
    "4. Linear scaling of the input features.\n",
    "    - No\n",
    "5. Adding zero-mean Gaussian noise to the training data or labels.\n",
    "    - Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Are SVM vulnerable to datasets like B? Why or Why not?\n",
    "Dataset B won't cause a problem for SVM because in SVM, it tries to maximize the geometric margin, which is already been \"normalized\" then won't be affected by scaling $\\left|\\left|\\theta\\right|\\right|$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
